# AUTOGENERATED! DO NOT EDIT! File to edit: ../nbs/source_nbs/00_data.ipynb.

# %% auto 0
__all__ = ['AD', 'load_data', 'ConditionalsTransform']

# %% ../nbs/source_nbs/00_data.ipynb 5
from fastai.vision.all import *
import andi_datasets

# %% ../nbs/source_nbs/00_data.ipynb 9
from andi_datasets.datasets_theory import datasets_theory
AD = datasets_theory()

# %% ../nbs/source_nbs/00_data.ipynb 22
from andi_datasets.utils_trajectories import normalize_fGN

# %% ../nbs/source_nbs/00_data.ipynb 23
def load_data(ds_args):
    if ds_args['model']=='fbm':
        ds = AD.create_dataset(T=ds_args["T"], N_models=ds_args["N"],
                               exponents=ds_args["alpha"], models=[2], 
                               N_save=6_000,#50_000,
                               t_save=400,
                               save_trajectories=False, load_trajectories=True,
                               path=ds_args["path"])  # trajs, T+2
        ds_labels = ds[:,1][:,None]  # trajs=N*n_alphas,1 # alpha
        n_labels = 1
        ds = ds[:,2:]  # trajs=N*n_alphas, T
        # convert trajectories to displacements:
        ds = np.subtract(ds[:,1:], ds[:,:-1])  # trajs=N*n_alphas, T-1
        # Normalize to the given D
        # take all alphas and assign one D each time
        ds = np.array([normalize_fGN(ds,ds_labels,D,399)
                       for D in ds_args['D']
                      ]).reshape(-1,ds_args["T"]-1)
        ds_labels =np.concatenate((np.tile(ds_labels, (len(ds_args['D']),1)),
                                   np.array(ds_args['D'])[:,None].repeat(len(ds_labels),axis=0)
                                  ),axis=-1)
    elif ds_args['model']=='sbm':
        ds_labels = np.empty((ds_args["N"]*len(ds_args["alpha"])*len(ds_args["D"]),2))
        ds        = np.empty((ds_args["N"]*len(ds_args["alpha"])*len(ds_args["D"]),ds_args["T"]-1))
        n = 0
        f = np.load('../../data/raw/sbm.npz')
        for i,a in enumerate(ds_args["alpha"]):
            for j,D in enumerate(ds_args["D"]):
                k = f'{a:.3g}'+f',{D:.3g}'
                ds_labels[n:n+ds_args["N"]] = f[k][:ds_args["N"],:2]
                ds[n:n+ds_args["N"]] = f[k][:ds_args["N"],2:2+ds_args["T"]-1]
                n =  n+ds_args["N"]
    # trajs=N*n_alphas, 2
    n_labels=2
    # ensure data type
    ds = ds.astype(np.float32);  ds_labels = ds_labels.astype(np.float32)
    ds = np.concatenate((ds_labels,ds),axis=-1)
    tfm = ConditionalsTransform(n_labels=n_labels)

    n_inp = 1
    splits_idx = RandomSplitter(valid_pct=ds_args['valid_pct'], seed=ds_args['seed'])(np.arange(len(ds)))
    tfl = TfmdLists(ds, tfm, splits=splits_idx,)  # iter
    return tfl.dataloaders(ds_args['bs'], n_inp=n_inp, num_workers=2, pin_memory=True, drop_last=True)

class ConditionalsTransform(Transform):
    def __init__(self, n_labels=1): store_attr()
    def encodes(self, andi_dataset_item):
        ds,ds_labels = andi_dataset_item[self.n_labels:], andi_dataset_item[:self.n_labels]
        # for convolutions insert a channel dimension.
        ds_ = np.expand_dims(ds, axis=-1)
        channel_dim = 0
        ds = np.expand_dims(ds, axis=channel_dim)
        return (ds, ds_)
