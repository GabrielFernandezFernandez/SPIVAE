{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b91900b-6e8c-456e-9a04-ec6871a49458",
   "metadata": {},
   "outputs": [],
   "source": [
    "#|default_exp utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d200fc55-44cf-4848-9412-b43a944eea69",
   "metadata": {},
   "outputs": [],
   "source": [
    "#|hide\n",
    "import nbdev; nbdev.nbdev_export() # C-s and C-RET to save and export"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26d282b3-1d3a-4f5c-a600-455e1f0cb193",
   "metadata": {},
   "source": [
    "# Utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df667b48-20b3-4791-9313-1f7adf3bab4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#|export\n",
    "from fastai.vision.all import *\n",
    "import andi_datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9c3b6e6-e431-4a67-96cd-bc1b403d0d03",
   "metadata": {},
   "source": [
    "## Loss function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "593db014-f461-4fbf-a791-a32de1429e80",
   "metadata": {},
   "outputs": [],
   "source": [
    "#|export\n",
    "# inspired from https://github.com/EtienneT/vae/blob/master/vae.ipynb\n",
    "# little trick to input the beta parameter in the loss and make a schedule for fastai.\n",
    "class Loss():\n",
    "    \"\"\"Loss function for a mixture of Gaussians\"\"\"\n",
    "    def __init__(self, RF, c, use_pad=False, beta=0, reduction='sum'): store_attr()\n",
    "    def __call__(self, pred, target,):\n",
    "        if   self.c > 0: y_hat, mu, logvar, c_ = pred # VAE + WaveNet\n",
    "        elif self.c== 0: y_hat                 = pred # WaveNet\n",
    "        elif self.c==-1: y_hat, mu, logvar     = pred # VAE\n",
    "        if self.use_pad:\n",
    "            log_likelihood = mix_gaussian_loss(y_hat, target, reduction=self.reduction)\n",
    "        else:\n",
    "            log_likelihood = mix_gaussian_loss(y_hat, target[:,self.RF:], reduction=self.reduction)\n",
    "\n",
    "        # return log_likelihood\n",
    "        if self.reduction=='none':\n",
    "            kl = kl_divergence(mu, logvar, reduction='none') if self.c else 0\n",
    "            return log_likelihood, self.beta, kl\n",
    "        else:\n",
    "            kl = kl_divergence(mu, logvar, reduction='meansum') if self.c else 0\n",
    "            return log_likelihood + self.beta*kl\n",
    "    def set_hyper(self, k, v):\n",
    "        self.k = v"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "def271a5-288a-4af3-ae03-6d9757469f3b",
   "metadata": {},
   "source": [
    "### Gaussian mixture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87336916-f3c1-489f-b6be-66fcadbdb67e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#|export\n",
    "from torch.distributions import Normal\n",
    "# we can easily define discretized version of the gaussian loss, however,\n",
    "# use continuous version as same as the https://clarinet-demo.github.io/\n",
    "\n",
    "def mix_gaussian_loss(y_hat, y, log_scale_min=-12.0, reduction='sum'):\n",
    "    \"\"\"Mixture of continuous Gaussian distributions loss.\n",
    "    Note that it is assumed that input is scaled to [-1, 1].\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "        y_hat (Tensor): Predicted output (B x C x T)\n",
    "        y (Tensor):     Target (B x T x 1).\n",
    "        log_scale_min (float): Log scale minimum value\n",
    "        reduce (bool):  If True, the losses are averaged or summed for each minibatch.\n",
    "          \n",
    "    Returns\n",
    "    -------\n",
    "        Tensor: loss\n",
    "    \"\"\"\n",
    "    ##  # BxCxHxW -> BxCxT\n",
    "    if y_hat.dim() == 4:\n",
    "        y_hat = y_hat.reshape(*y_hat.shape[:2], -1)\n",
    "    if y.dim() == 4:\n",
    "        # BxCxHxW -> BxCxT\n",
    "        y = y.reshape(*y.shape[:2], -1)\n",
    "        # BxCxT -> BxTxC\n",
    "        y = y.transpose(1, 2)\n",
    "    ##\n",
    "\n",
    "    assert y_hat.dim() == 3, f\"y_hat {y_hat.dim()} \" + f\"{y_hat.shape}\"\n",
    "    C = y_hat.size(1)\n",
    "    if C == 2:\n",
    "        nr_mix = 1\n",
    "    else:\n",
    "        assert y_hat.size(1) % 3 == 0\n",
    "        nr_mix = y_hat.size(1) // 3\n",
    "\n",
    "    channel_dim = 1\n",
    "    # BxCxT -> BxTxC\n",
    "    y_hat = y_hat.transpose(1, 2)\n",
    "    channel_dim = 2\n",
    "\n",
    "    # unpack parameters.\n",
    "    if C == 2:\n",
    "        # special case for C == 2, just for compatibility\n",
    "        logit_probs = None\n",
    "        means = y_hat[:, :, 0:1]\n",
    "        log_scales = torch.clamp(y_hat[:, :, 1:2], min=log_scale_min) # clamp removable?\n",
    "    else:\n",
    "        #  (B, T, num_mixtures) x 3\n",
    "        logit_probs = y_hat[:, :, :nr_mix]\n",
    "        means = y_hat[:, :, nr_mix:2 * nr_mix]\n",
    "        log_scales = torch.clamp(y_hat[:, :, 2 * nr_mix:3 * nr_mix],\n",
    "                                 min=log_scale_min)\n",
    "\n",
    "    # repeat to match all the distributions in the mixture\n",
    "    # B x T x 1 -> B x T x num_mixtures\n",
    "    y = y.expand_as(means)\n",
    "\n",
    "    centered_y = y - means\n",
    "    dist = Normal(loc=0., scale=torch.exp(log_scales))\n",
    "\n",
    "    log_probs = dist.log_prob(centered_y)\n",
    "\n",
    "    if nr_mix > 1:\n",
    "        log_probs = log_probs + F.log_softmax(logit_probs, -1) # mixture probabilities are added: log P(x_{t=1}^{(i=1)})+log P_{jt}^{(i)}\n",
    "\n",
    "    if reduction == 'mean':\n",
    "        if nr_mix == 1:\n",
    "            # NLL\n",
    "            return -torch.mean(log_probs) # sum in every dimension (B, T, num_mixtures) and divide by the number of elements, return an scalar\n",
    "        else:\n",
    "#             return -torch.sum(log_sum_exp(log_probs))\n",
    "            return -torch.mean(torch.logsumexp(log_probs, channel_dim)) \n",
    "            # logsumexp in channel/mixtures dimension and then sum everything else\n",
    "    elif reduction == 'sum':\n",
    "        if nr_mix == 1:\n",
    "            # NLL\n",
    "            return -torch.sum(log_probs) # sum in every dimension (B, T, num_mixtures) and return an scalar\n",
    "        else:\n",
    "#             return -torch.sum(log_sum_exp(log_probs))\n",
    "            return -torch.sum(torch.logsumexp(log_probs, channel_dim))\n",
    "            # logsumexp in channel/mixtures dimension and then sum everything else\n",
    "    else:\n",
    "        if nr_mix == 1:\n",
    "            return -log_probs\n",
    "        else:\n",
    "#             return -log_sum_exp(log_probs).unsqueeze(-1)\n",
    "            return -torch.logsumexp(log_probs, channel_dim).unsqueeze(-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "466170f8-8a9e-4458-a137-95d361cbcac9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#|export\n",
    "def sample_from_mix_gaussian(y, log_scale_min=-12.0):\n",
    "    \"\"\"\n",
    "    Sample from (discretized) mixture of gaussian distributions.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "        y (Tensor): B x C x T\n",
    "        log_scale_min (float): Log scale minimum value\n",
    "        \n",
    "    Returns\n",
    "    -------\n",
    "        Tensor\n",
    "    \"\"\"\n",
    "    C = y.size(1)\n",
    "    if C == 2:\n",
    "        nr_mix = 1\n",
    "    else:\n",
    "        assert y.size(1) % 3 == 0\n",
    "        nr_mix = y.size(1) // 3\n",
    "\n",
    "    # B x T x C\n",
    "    y = y.transpose(1, 2)\n",
    "\n",
    "    if C == 2:\n",
    "        logit_probs = None\n",
    "    else:\n",
    "        logit_probs = y[:, :, :nr_mix]\n",
    "\n",
    "    if nr_mix > 1:\n",
    "        # sample mixture indicator from softmax\n",
    "        temp = logit_probs.data.new(logit_probs.size()).uniform_(\n",
    "            1e-5, 1.0 - 1e-5)\n",
    "        temp = logit_probs.data - torch.log(-torch.log(temp))\n",
    "        _, argmax = temp.max(dim=-1)\n",
    "\n",
    "        # (B, T) -> (B, T, nr_mix)\n",
    "        one_hot = to_one_hot(argmax, nr_mix)\n",
    "\n",
    "        # Select means and log scales\n",
    "        means = torch.sum(y[:, :, nr_mix:2 * nr_mix] * one_hot, dim=-1)\n",
    "        log_scales = torch.sum(y[:, :, 2 * nr_mix:3 * nr_mix] * one_hot,\n",
    "                               dim=-1)\n",
    "    else:\n",
    "        if C == 2:\n",
    "            means, log_scales = y[:, :, 0], y[:, :, 1]\n",
    "        elif C == 3:\n",
    "            means, log_scales = y[:, :, 1], y[:, :, 2]\n",
    "        else:\n",
    "            assert False, f\"shouldn't happen C={C}\"\n",
    "\n",
    "    scales = torch.exp(log_scales)\n",
    "    dist = Normal(loc=means, scale=scales)\n",
    "    x = dist.sample()\n",
    "    # x = torch.clamp(x, min=-1.0, max=1.0) # [-1,1] removed to have no truncation in sampling\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa8fe23d-34e7-4f5c-93cb-a7189c6a855c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#|export\n",
    "def min_nll_BM(D): return 0.5*np.log(4*np.pi*D)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20faa1a3-8f37-49b2-8685-3ab8810779ff",
   "metadata": {},
   "source": [
    "### Kullback-Leibler Divergence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c048512-6a8a-43c5-a3fc-59f152cee75e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#|export\n",
    "def kl_divergence(mu, logvar, reduction='meansum'):\n",
    "    \"\"\"\n",
    "    Computes the divergence term of the VAE loss function.\n",
    "    \"\"\"\n",
    "    klds = -0.5*(1. + logvar - mu.pow(2) - logvar.exp()) # batch_size, z_dim\n",
    "    if reduction == 'meansum':\n",
    "        # total_kld\n",
    "        kld = klds.sum(1).mean(0)\n",
    "        # sum of dimension_wise_kld # size 1 \n",
    "    elif reduction == 'mean':\n",
    "        kld = klds.mean()\n",
    "    elif reduction == 'sum':\n",
    "        kld = klds.sum()\n",
    "    elif reduction == 'meanb': # dimension_wise_kld\n",
    "        kld = klds.mean(0)\n",
    "    else:\n",
    "        kld = klds\n",
    "\n",
    "    # sanity check\n",
    "    assert torch.all(torch.isfinite(kld)), f\"Non finite value found on KLD loss computation: {kld.item()}.\"\n",
    "    f'May {logvar=} be too big/small?'\n",
    "    return kld"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "075899e6-5e40-414d-b763-f086bcd33de4",
   "metadata": {},
   "source": [
    "## Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a9855c4-e59a-471c-a995-8a8663e3f15d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#|export\n",
    "# inspired from https://github.com/EtienneT/vae/blob/master/vae.ipynb\n",
    "# and https://github.com/fastai/fastai/blob/master/fastai/learner.py#L467\n",
    "class GaussianMixtureMetric(AvgMetric):\n",
    "    \"\"\"Metric to log the Gaussian mixture loss\"\"\"\n",
    "    def __init__(self, RF:int,c,use_pad:bool=False,func=mix_gaussian_loss, reduction='mean'): store_attr()\n",
    "    def accumulate(self, learn):\n",
    "        bs = find_bs(learn.yb)\n",
    "        if   self.c > 0: pred, mu, logvar, c_ = to_detach(learn.pred, cpu=False) # VAE + WaveNet\n",
    "        elif self.c== 0: pred                 = to_detach(learn.pred, cpu=False) # WaveNet\n",
    "        elif self.c==-1: pred, mu, logvar     = to_detach(learn.pred, cpu=False) # VAE\n",
    "        gaussian_mix_loss = to_detach(self.func(pred, learn.y[:,self.RF:], reduction=self.reduction).mean(), cpu=True)\n",
    "        if self.use_pad:\n",
    "            gaussian_mix_loss = to_detach(self.func(pred, learn.y, reduction=self.reduction).mean(), cpu=True)\n",
    "        else:\n",
    "            gaussian_mix_loss = to_detach(self.func(pred, learn.y[:,self.RF:], reduction=self.reduction).mean(), cpu=True)\n",
    "        self.total += gaussian_mix_loss*bs\n",
    "        self.count += bs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "259cec0e-7a14-445e-be9d-fd759617c6d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#|export\n",
    "class KLDMetric(AvgMetric):\n",
    "    \"\"\"Metric to log the Kullback-Leibler divergence term\"\"\"\n",
    "    def __init__(self, c): store_attr()\n",
    "    def accumulate(self, learn):\n",
    "        bs = find_bs(learn.yb)\n",
    "        if   self.c > 0: pred, mu, logvar, c_ = to_detach(learn.pred, cpu=False) # VAE + WaveNet\n",
    "        elif self.c==-1: pred, mu, logvar     = to_detach(learn.pred, cpu=False) # VAE\n",
    "        KLD = -0.5*(1 + logvar - mu.pow(2) - logvar.exp()).sum(1).mean(0) if self.c and mu is not None else 0 # batch_size, z_dim\n",
    "        KLD = to_detach(KLD, cpu=True)\n",
    "        self.total += KLD*bs\n",
    "        self.count += bs\n",
    "    @property\n",
    "    def name(self):  return 'kld'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae9c8498-0731-42a0-b742-4f0f9b3d9ba9",
   "metadata": {},
   "source": [
    "## Learner Callbacks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a6132fc-983c-4071-8d26-4aeb75916a2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#|export\n",
    "class ShowLossCallback(Callback):\n",
    "    \"Update a graph of training and validation loss\"\n",
    "    order,run_valid=65,False\n",
    "    def __init__(self, title): self.title=title\n",
    "    def before_fit(self):\n",
    "        self.run = not hasattr(self.learn, 'lr_finder') and not hasattr(self, \"gather_preds\")\n",
    "        if not(self.run): return\n",
    "        self.nb_batches = []\n",
    "        assert hasattr(self.learn, 'progress')\n",
    "\n",
    "    def after_train(self):  self.nb_batches.append(self.train_iter)\n",
    "\n",
    "    def after_epoch(self):\n",
    "        \"Plot validation loss in the pbar graph\"\n",
    "        if not self.nb_batches: return\n",
    "        rec = self.learn.recorder\n",
    "        iters = range_of(rec.losses)\n",
    "        val_losses = [v[1] for v in rec.values]\n",
    "        # no bounds to see fluctuation in real time\n",
    "        x_bounds = None # (0, (self.n_epoch - len(self.nb_batches)) * self.nb_batches[0] + len(rec.losses)) # None\n",
    "        y_bounds = None # (min((min(Tensor(rec.losses)), min(Tensor(val_losses)))),\n",
    "                    # max((max(Tensor(rec.losses)), max(Tensor(val_losses)))))\n",
    "        self.progress.mbar.update_graph([(iters, rec.losses), (self.nb_batches, val_losses)], x_bounds, y_bounds)\n",
    "\n",
    "        self.progress.mbar.graph_ax.set_xlabel('Batches', x=-0.3, horizontalalignment='left', verticalalignment='bottom')\n",
    "        self.progress.mbar.graph_ax.set_ylabel('Loss')\n",
    "        self.progress.mbar.graph_ax.set_title(self.title)\n",
    "        # Epochs x axis\n",
    "        # global batches_per_epoch\n",
    "        if self.nb_batches:\n",
    "            batches_per_epoch = self.nb_batches[0]# [-1]/self.n_epoch\n",
    "            def epoch2batch(epoch):    return epoch * batches_per_epoch\n",
    "            def batch2epoch(batch):    return batch / batches_per_epoch\n",
    "            secax = self.progress.mbar.graph_ax.secondary_xaxis(-0.2, functions=(batch2epoch, epoch2batch))\n",
    "            secax.set_xlabel('Epochs', x=-0.3, horizontalalignment='left', verticalalignment='bottom');\n",
    "            self.progress.mbar.graph_out.update(self.progress.mbar.graph_ax.figure)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f206a71-4b54-4638-9ba6-78d5c86b9025",
   "metadata": {},
   "outputs": [],
   "source": [
    "#|export\n",
    "class ShowKLDsCallback(Callback):\n",
    "    \"Update a graph of training and validation loss\"\n",
    "    order,run_valid=65,False\n",
    "    names = ['']\n",
    "    def __init__(self, title):\n",
    "        self.title=title\n",
    "\n",
    "    def before_fit(self):\n",
    "        self.run = not hasattr(self.learn, 'lr_finder') and not hasattr(self, \"gather_preds\")\n",
    "        if not(self.run): return\n",
    "        self.nb_batches = []\n",
    "        assert hasattr(self.learn, 'progress')\n",
    "\n",
    "    def after_validate(self):  self.on_iter_end()\n",
    "\n",
    "    def after_fit(self):\n",
    "        self.on_iter_end()\n",
    "        if hasattr(self, 'graph_fig'): delattr(self, 'graph_fig')\n",
    "        # needed to recreate the graph when fitting again\n",
    "        # https://github.com/fastai/fastai/blob/master/fastai/callback/progress.py#L39\n",
    "\n",
    "    def after_train(self):    self.on_iter_end();    self.nb_batches.append(self.train_iter)\n",
    "\n",
    "    def on_iter_end(self):\n",
    "        # from https://github.com/fastai/fastprogress/blob/master/fastprogress/fastprogress.py#L155\n",
    "        if hasattr(self, 'imgs_fig'):\n",
    "            plt.close()\n",
    "            self.imgs_out.update(self.imgs_fig)\n",
    "        if hasattr(self, 'graph_fig'):\n",
    "            plt.close()\n",
    "            self.graph_out.update(self.graph_fig)\n",
    "\n",
    "    def after_epoch(self):\n",
    "        \"Plot validation loss in the pbar graph\"\n",
    "        if not self.nb_batches: return\n",
    "        klds = np.stack(self.learn.kl_ds.preds)\n",
    "        start_b = 0#17500\n",
    "        # plt.yscale('symlog', linthresh=1e-8);\n",
    "        # no bounds to see fluctuation in real time\n",
    "        x_bounds = None # (0, (self.n_epoch - len(self.nb_batches)) * self.nb_batches[0] + len(rec.losses)) # None\n",
    "        y_bounds = None # (min((min(Tensor(rec.losses)), min(Tensor(val_losses)))),\n",
    "                    # max((max(Tensor(rec.losses)), max(Tensor(val_losses)))))\n",
    "        graphs = [(range(start_b,len(klds)),klds[start_b:])]\n",
    "        self.update_graph(graphs, x_bounds, y_bounds)\n",
    "\n",
    "    def update_graph(self, graphs, x_bounds=None, y_bounds=None, figsize=(6,4)):\n",
    "        if not hasattr(self, 'graph_fig'):\n",
    "            self.graph_fig, self.graph_ax = plt.subplots(1, figsize=figsize)\n",
    "            self.graph_out = display(self.graph_ax.figure, display_id=True)\n",
    "        self.graph_ax.clear()\n",
    "        if len(self.names) < len(graphs): self.names += [''] * (len(graphs) - len(self.names))\n",
    "        for g,n in zip(graphs,self.names): self.graph_ax.plot(*g, label=n)\n",
    "        h,l = self.graph_ax.get_legend_handles_labels()\n",
    "        if h: self.graph_ax.legend(h,l,loc='upper right')\n",
    "        if x_bounds is not None: self.graph_ax.set_xlim(*x_bounds)\n",
    "        if y_bounds is not None: self.graph_ax.set_ylim(*y_bounds)\n",
    "\n",
    "        self.graph_ax.set_xlabel('Batches', x=-0.3, horizontalalignment='left', verticalalignment='bottom')\n",
    "        self.graph_ax.set_ylabel(r'DKL')\n",
    "        self.graph_ax.grid()\n",
    "        self.graph_ax.set_yscale('log')\n",
    "        self.graph_ax.set_title(self.title+ r', \\(\\beta=\\)' + f'{self.learn.model.beta}')\n",
    "        # Epochs x axis\n",
    "        # global batches_per_epoch\n",
    "        if self.nb_batches:\n",
    "            batches_per_epoch = self.nb_batches[0]\n",
    "            def epoch2batch(epoch):    return epoch * batches_per_epoch\n",
    "            def batch2epoch(batch):    return batch / batches_per_epoch\n",
    "            secax = self.graph_ax.secondary_xaxis(-0.2, functions=(batch2epoch, epoch2batch))\n",
    "            secax.set_xlabel('Epochs', x=-0.3, horizontalalignment='left', verticalalignment='bottom');\n",
    "        self.graph_out.update(self.graph_ax.figure)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d650c358-f166-4604-bf88-7c16d303de40",
   "metadata": {},
   "outputs": [],
   "source": [
    "#|export\n",
    "class ShowLatentsCallback(ShowKLDsCallback):\n",
    "    \"Update a graph of latent space\"\n",
    "    order,run_valid=65,False\n",
    "    names = ['']\n",
    "    def __init__(self, c, title): store_attr()\n",
    "\n",
    "    def before_fit(self):\n",
    "        self.run = not hasattr(self.learn, 'lr_finder') and not hasattr(self, \"gather_preds\")\n",
    "        if not(self.run): return\n",
    "        self.nb_batches = []\n",
    "        assert hasattr(self.learn, 'progress')\n",
    "        # get idx\n",
    "        alphas_items = self.learn.dls.valid.items[:,0]\n",
    "        Ds_items     = self.learn.dls.valid.items[:,1]\n",
    "        u_a=np.unique(alphas_items, return_index=True, return_inverse=True)\n",
    "        u_D=np.unique(Ds_items, return_index=True, return_inverse=True)\n",
    "        alphas_idx = [np.flatnonzero(alphas_items==a) for a in u_a[0]]\n",
    "        Ds_idx = [np.flatnonzero(Ds_items==D) for D in u_D[0]]\n",
    "\n",
    "        intersect_idx = []\n",
    "        for i,a in enumerate(u_a[0]):\n",
    "            for j,D in enumerate(u_D[0]):\n",
    "                intersect_idx.append(np.intersect1d(alphas_idx[i],Ds_idx[j]))\n",
    "\n",
    "        self.intersect_idx = np.array(intersect_idx).reshape(len(u_a[0]),len(u_D[0]))\n",
    "        self.alphas_idx_flat = [item for sublist in alphas_idx for item in sublist]\n",
    "        self.Ds_idx_flat = [item for sublist in Ds_idx for item in sublist]\n",
    "\n",
    "    def after_epoch(self):\n",
    "        \"Plot validation loss in the pbar graph\"\n",
    "        if not self.nb_batches: return\n",
    "\n",
    "        cb = GatherPredsCallback(with_targs=False)\n",
    "        ctx_mgrs = self.validation_context(cbs=L(cb), inner=True)\n",
    "        dl = self.learn.dls[1].new(shuffle=False, drop_last=False) if self.learn.dls[1].drop_last else self.learn.dls[1]\n",
    "        with ContextManagers(ctx_mgrs):\n",
    "            self._do_epoch_validate(dl=dl)\n",
    "            preds, _ = cb.all_tensors()\n",
    "        if self.c >0: pred, mu, logvar, c = to_detach(preds)\n",
    "        elif self.c==-1: pred, mu, logvar = to_detach(preds)\n",
    "\n",
    "        # plot\n",
    "        # no bounds to see fluctuation in real time\n",
    "        x_bounds = None; y_bounds = None\n",
    "        graphs = [(range(len(self.alphas_idx_flat)),mu[self.alphas_idx_flat])]\n",
    "        self.update_graph(graphs, x_bounds, y_bounds)\n",
    "\n",
    "    def update_graph(self, graphs, x_bounds=None, y_bounds=None, figsize=(6,4)):\n",
    "        if not hasattr(self, 'graph_fig'):\n",
    "            self.graph_fig, self.graph_ax = plt.subplots(1, figsize=figsize)\n",
    "            self.graph_out = display(self.graph_ax.figure, display_id=True)\n",
    "        self.graph_ax.clear()\n",
    "        if len(self.names) < len(graphs): self.names += [''] * (len(graphs) - len(self.names))\n",
    "        for g,n in zip(graphs,self.names): self.graph_ax.plot(*g, label=n)\n",
    "        h,l = self.graph_ax.get_legend_handles_labels()\n",
    "        if h: self.graph_ax.legend(h,l,loc='upper right')\n",
    "        if x_bounds is not None: self.graph_ax.set_xlim(*x_bounds)\n",
    "        if y_bounds is not None: self.graph_ax.set_ylim(*y_bounds)\n",
    "\n",
    "        self.graph_ax.set_xlabel('ordered a index',)\n",
    "        self.graph_ax.set_ylabel(r'\\(\\mu\\) latent neuron activation')\n",
    "        self.graph_ax.grid()\n",
    "        # self.graph_ax.set_yscale('log')\n",
    "        self.graph_ax.set_title(self.title)\n",
    "        self.graph_out.update(self.graph_ax.figure)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd530d50-ef79-4a8e-b76b-c6ca6920a0ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "#|export\n",
    "class KLDsCallback(Callback):\n",
    "    \"Record KLD per latent variable\"\n",
    "    order,run_valid=65,False\n",
    "    def __init__(self,c): store_attr(); self.preds = []\n",
    "    def after_batch(self):\n",
    "        if   self.c > 0: pred, mu, logvar, c_ = to_detach(self.learn.pred, cpu=False) # VAE + WaveNet\n",
    "        elif self.c==-1: pred, mu, logvar     = to_detach(self.learn.pred, cpu=False) # VAE\n",
    "        KLD = -0.5*(1 + logvar - mu.pow(2) - logvar.exp()).mean(0) if mu is not None else 0. # z_dim\n",
    "        self.preds.append(KLD) # z_dim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37587298-94ff-4345-b081-2d844230dcda",
   "metadata": {},
   "outputs": [],
   "source": [
    "#|export\n",
    "def plot_klds(learn, start_b=0, title=''):\n",
    "    klds = learn.model.beta*np.stack(learn.kl_ds.preds) if learn.model.beta!=0. else np.stack(learn.kl_ds.preds)\n",
    "    plt.semilogy(range(start_b,len(klds)),klds[start_b:]);\n",
    "    plt.xlabel('Batches');plt.ylabel(r'\\(\\beta\\) DKL');\n",
    "    plt.title(title + r', \\(\\beta=\\)' + f'{learn.model.beta}');\n",
    "    plt.grid();\n",
    "    if learn.model.beta>0. and hasattr(learn, 'loss_param_scheduler'): plt.plot(learn.loss_param_scheduler.hps['beta'], alpha=0.5);\n",
    "    plt.yscale('symlog', linthresh=1e-8);\n",
    "    return plt.gcf()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6fc733a-9bcc-4c4c-901a-40a34243cff8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#|export\n",
    "class GMsCallback(Callback):\n",
    "    \"Record NLL gaussian mixture log-likelihood means per alpha and D during training\"\n",
    "    order,run_valid=65,False\n",
    "    def __init__(self,RF,c,use_pad=False,reduction='none'): store_attr(); self.preds = [];\n",
    "    def after_pred(self):\n",
    "        if   self.c > 0: pred, mu, logvar, c_ = to_detach(self.learn.pred, cpu=False) # VAE + WaveNet\n",
    "        elif self.c== 0: pred                 = to_detach(self.learn.pred, cpu=False) # WaveNet\n",
    "        elif self.c==-1: pred, mu, logvar     = to_detach(self.learn.pred, cpu=False) # VAE\n",
    "        if self.use_pad:\n",
    "            gaussian_mix_loss = mix_gaussian_loss(pred, self.learn.y, reduction=self.reduction)\n",
    "        else:\n",
    "            gaussian_mix_loss = mix_gaussian_loss(pred, self.learn.y[:,self.RF:], reduction=self.reduction)\n",
    "        gaussian_mix_loss = to_detach(gaussian_mix_loss, cpu=True)\n",
    "        # get idx\n",
    "        i = self.learn.iter # current iteration, batch number\n",
    "        current_items = self.learn.dls.items[ self.learn.dls.train._DataLoader__idxs[i*256:(i+1)*256]]\n",
    "        alphas_items, Ds_items = current_items[:,0], current_items[:,1]\n",
    "        u_a=np.unique(alphas_items);  u_D=np.unique(Ds_items)\n",
    "        alphas_idx = [np.flatnonzero(alphas_items==a) for a in u_a]\n",
    "        Ds_idx = [np.flatnonzero(Ds_items==D) for D in u_D]\n",
    "\n",
    "        intersect_idx = np.array([[np.intersect1d(alphas_idx[i],Ds_idx[j]) for j,D in enumerate(u_D)] for i,a in enumerate(u_a)])\n",
    "\n",
    "        gaussian_mix_loss_mean_aD = np.array([\n",
    "            [gaussian_mix_loss[intersect_idx[i,j]].mean()\n",
    "             for j,D in enumerate(u_D)]\n",
    "            for i,a in enumerate(u_a)] )\n",
    "\n",
    "        self.preds.append(gaussian_mix_loss_mean_aD)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0888376a-1e55-4b65-a059-d2023ea66374",
   "metadata": {},
   "source": [
    "# Anomalous Diffusion"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc29bd89-7f23-4af5-9c54-870a972013c7",
   "metadata": {},
   "source": [
    "## Correlations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aaa55308-de67-40e9-b077-bf6302f32fed",
   "metadata": {},
   "outputs": [],
   "source": [
    "#|export\n",
    "def correlations(traj, windows, subtract=True, verbose=True):\n",
    "    if subtract: # if the input are trajectories then transform to displacements\n",
    "        displacements = np.subtract(traj[:,1:],traj[:,:-1]) # traj, T-1\n",
    "    else:\n",
    "        displacements = traj\n",
    "    if verbose: print(f\"{displacements.shape=}\")\n",
    "    # mean correlations\n",
    "    corr_mean = np.array([np.multiply(displacements[:,:-win],displacements[:,win:]).mean(-1) for win in range(1,windows)]) # much less memory consumption\n",
    "    if verbose: print(f\"{corr_mean.shape=}\")\n",
    "    return corr_mean\n",
    "\n",
    "def correlations_plot(traj, windows, title, displacements=True, fbm=True):\n",
    "    corr_mean = correlations(traj, windows, subtract=not displacements)\n",
    "    corr_mean_traj = corr_mean.mean(-1) # , windows traj\n",
    "    delta_t = range(1,windows)\n",
    "    # plot\n",
    "    if fbm:\n",
    "        #displacements_corr_mean, displacements_corr_mean_traj, displacements_corr_fig\n",
    "        plt.loglog(delta_t, np.abs(corr_mean_traj)/np.abs(corr_mean_traj[0]), '-+', label=\"Generated\");\n",
    "        alpha=0.4\n",
    "        D=2e-2  # 0.015169\n",
    "        plt.plot(delta_t, np.abs(alpha*(alpha-1)*D*np.arange(1,windows)**(alpha-2))/np.abs(alpha*(alpha-1)*D), label=r'$\\alpha =' f'{alpha}' \n",
    "                 # r', ' f'{D=}'\n",
    "                 r'$')\n",
    "        alpha=1.7\n",
    "        D=2e-2  # 0.015169\n",
    "        plt.plot(delta_t, np.abs(alpha*(alpha-1)*D*np.arange(1,windows)**(alpha-2))/np.abs(alpha*(alpha-1)*D), label=r'$\\alpha =' f'{alpha}'\n",
    "                 # r', ' f'{D=}' \n",
    "                 r'$')\n",
    "        plt.ylabel(r\"Normalized Absolute Mean of products $\\left|\\langle d_td_{t+\\Delta t} \\rangle\\right|/d_t^2$\");\n",
    "    else:\n",
    "        plt.plot(delta_t, corr_mean, '+', alpha=0.1);\n",
    "        plt.plot(delta_t, corr_mean_traj, '-+', label=r\"$\\mu$ reconstructed\");\n",
    "        plt.ylabel(r\"Mean of products $\\langle d_td_{t+\\Delta t} \\rangle$\");\n",
    "        plt.axhline(0, c='black', alpha=0.5)\n",
    "    plt.xlabel(\"Time windows length \" r\" $\\Delta t$\");\n",
    "    plt.title(title); #f\"1D Brownian D={1}, T=200, N={corr_mean.shape[-1]:e} \" r\"$\\mu$\");\n",
    "    plt.legend(loc='lower left');\n",
    "    plt.grid();\n",
    "    return corr_mean, corr_mean_traj, plt.gcf()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8f5e2dd-6988-43eb-9c98-f74c19d6c79a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#|export\n",
    "def sig2D(sigma):\n",
    "    return sigma**2*0.5\n",
    "def D2sig(D):\n",
    "    return (2*D)**0.5"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72995f67-8dc1-4638-a96e-719c93ab2545",
   "metadata": {},
   "source": [
    "## TAMSD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffac37e7-aa4c-4119-86d3-b36a3e2e24e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#|export\n",
    "from numba import njit, prange, set_num_threads\n",
    "set_num_threads(2)\n",
    "\n",
    "def TMSD_array_(trajs, t_lags):\n",
    "    ttt = np.zeros((trajs.shape[0], len(t_lags))) # N trajs, windows\n",
    "    for idx, t in enumerate(t_lags):\n",
    "        for p in range(trajs.shape[1]-t):\n",
    "            ttt[:, idx] += (trajs[:, p]-trajs[:, p+t])**2\n",
    "        ttt[:, idx] /= trajs.shape[1]-t\n",
    "    return ttt\n",
    "\n",
    "@njit\n",
    "def TAMSD_(traj, t_lags):\n",
    "    tamsd = np.zeros_like(t_lags, dtype=np.float64)\n",
    "    for idx, t in enumerate(t_lags):\n",
    "        for p in range(len(traj) - t):\n",
    "            tamsd[idx] += (traj[p] - traj[p + t])**2\n",
    "        tamsd[idx] /= len(traj) - t\n",
    "    return tamsd\n",
    "\n",
    "@njit(parallel=True)\n",
    "def TMSD_array(trajs, t_lags):\n",
    "    ttt = np.zeros((trajs.shape[0], len(t_lags)), dtype=np.float64)\n",
    "    for i in prange(trajs.shape[0]):\n",
    "        ttt[i] = TAMSD_(trajs[i], t_lags)\n",
    "    return ttt\n",
    "\n",
    "def TAMSD(trajs, t_lags):\n",
    "    return TMSD_array(trajs, t_lags)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9718da4e-dce6-4a58-9c11-1772c6ac2530",
   "metadata": {},
   "source": [
    "## TEAMSD"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1147c4f1-cdfd-4beb-9f37-e0d89e3b14f3",
   "metadata": {},
   "source": [
    "# Save/Load models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5df5fc9a-bc45-406a-8eeb-44ed37f55bef",
   "metadata": {},
   "outputs": [],
   "source": [
    "#|export\n",
    "def save_model(fpath, model, model_args, ds_args, train_args):\n",
    "    state = dict(model_args=model_args, ds_args=ds_args,train_args=train_args,\n",
    "                 network_state_dict=model.state_dict(),\n",
    "                 #optimizer_state_dict=model.optimizer.state_dict()\n",
    "                )\n",
    "    torch.save(state, f'{fpath}.tar')\n",
    "    print('Saved at '+ f'{fpath}.tar')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc3f006a-1a70-42dd-b821-06cbba409b85",
   "metadata": {},
   "outputs": [],
   "source": [
    "# save_model(\"./models/fbm\", model_args, ds_args, train_args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ceed4a9d-53b0-4f95-9948-a66424da4144",
   "metadata": {},
   "outputs": [],
   "source": [
    "#|export\n",
    "def load_checkpoint(fpath, model_class, device='cuda'):\n",
    "    try: # device\n",
    "        checkpoint = torch.load(f'{fpath}.tar', map_location=device)\n",
    "    except: # use CPU\n",
    "        checkpoint = torch.load(f'{fpath}.tar', map_location='cpu')\n",
    "        print(f\"Loading into CPU from {device}\")\n",
    "        device = 'cpu'\n",
    "\n",
    "    print(\"Loading checkpoint: \"+f\"{fpath}.tar\")\n",
    "\n",
    "    network = model_class(**checkpoint['model_args']).to(device)\n",
    "    optimizer = torch.optim.Adam(network.parameters(), lr=checkpoint['train_args']['lr'])\n",
    "    try:\n",
    "        network.load_state_dict(checkpoint['network_state_dict'])\n",
    "    except:\n",
    "        raise RuntimeError(\"Error loading the checkpoint\")\n",
    "    return checkpoint, network, optimizer"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
