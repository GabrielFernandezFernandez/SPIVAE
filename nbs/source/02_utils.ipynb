{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b91900b-6e8c-456e-9a04-ec6871a49458",
   "metadata": {},
   "outputs": [],
   "source": [
    "#|default_exp utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d200fc55-44cf-4848-9412-b43a944eea69",
   "metadata": {},
   "outputs": [],
   "source": [
    "#|hide\n",
    "import nbdev; nbdev.nbdev_export() # C-s and C-RET to save and export"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "e2d8ce8b-79ac-4e36-8bc4-130b4e064fb9",
   "metadata": {},
   "source": [
    "# Utils\n",
    "> Functions for the common machine learning utilities plus diffusion related methods."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df667b48-20b3-4791-9313-1f7adf3bab4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#|export\n",
    "from fastai.vision.all import *\n",
    "import andi_datasets\n",
    "from SPIVAE.models import VAEWaveNet"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9c3b6e6-e431-4a67-96cd-bc1b403d0d03",
   "metadata": {},
   "source": [
    "## Loss function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66cbf264-254f-4644-9533-81592221737b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#|export\n",
    "from torch.distributions import Normal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "593db014-f461-4fbf-a791-a32de1429e80",
   "metadata": {},
   "outputs": [],
   "source": [
    "#|export\n",
    "# inspired from https://github.com/EtienneT/vae/blob/master/vae.ipynb\n",
    "# little trick to input the beta parameter in the loss and make a schedule for fastai.\n",
    "class Loss():\n",
    "    \"\"\"Î²-VAE loss function for a reconstruction term that is the negative log-likelihood\n",
    "    of a mixture of Gaussians\"\"\"\n",
    "    def __init__(self, RF, c, use_pad=False, beta=0, reduction='sum'): store_attr()\n",
    "    def __call__(self, pred, target,):\n",
    "        if   self.c > 0: y_hat, mu, logvar, c_ = pred # VAE + WaveNet\n",
    "        elif self.c== 0: y_hat                 = pred # WaveNet\n",
    "        elif self.c==-1: y_hat, mu, logvar     = pred # VAE\n",
    "        if self.use_pad:\n",
    "            log_likelihood = mix_gaussian_loss(y_hat, target, reduction=self.reduction)\n",
    "        else:\n",
    "            log_likelihood = mix_gaussian_loss(y_hat, target[:,self.RF:], reduction=self.reduction)\n",
    "\n",
    "        if self.reduction=='none':\n",
    "            kl = kl_divergence(mu, logvar, reduction='none') if self.c else 0\n",
    "            return log_likelihood, self.beta, kl\n",
    "        else:\n",
    "            kl = kl_divergence(mu, logvar, reduction='meansum') if self.c else 0\n",
    "            return log_likelihood + self.beta*kl\n",
    "\n",
    "    def set_hyper(self, k, v):\n",
    "        self.k = v"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "def271a5-288a-4af3-ae03-6d9757469f3b",
   "metadata": {},
   "source": [
    "### Gaussian mixture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87336916-f3c1-489f-b6be-66fcadbdb67e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#|export\n",
    "# we can easily define discretized version of the gaussian loss, however,\n",
    "# use continuous version as same as the https://clarinet-demo.github.io/\n",
    "\n",
    "def mix_gaussian_loss(y_hat, y, log_scale_min=-12.0, reduction='sum'):\n",
    "    \"\"\"Mixture of continuous Gaussian distributions loss.\n",
    "    Note that it is assumed that input is scaled to [-1, 1].\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "        y_hat (Tensor):\n",
    "            Predicted output (B x C x T)\n",
    "        y (Tensor):\n",
    "            Target (B x T x 1).\n",
    "        log_scale_min (float):\n",
    "            Log scale minimum value\n",
    "        reduce (bool): \n",
    "            If True, the losses are averaged or summed for each minibatch.\n",
    "          \n",
    "    Returns\n",
    "    -------\n",
    "        Tensor\n",
    "            Loss\n",
    "    \"\"\"\n",
    "    ##  # BxCxHxW -> BxCxT\n",
    "    if y_hat.dim() == 4:\n",
    "        y_hat = y_hat.reshape(*y_hat.shape[:2], -1)\n",
    "    if y.dim() == 4:\n",
    "        # BxCxHxW -> BxCxT\n",
    "        y = y.reshape(*y.shape[:2], -1)\n",
    "        # BxCxT -> BxTxC\n",
    "        y = y.transpose(1, 2)\n",
    "    ##\n",
    "\n",
    "    assert y_hat.dim() == 3, f\"y_hat {y_hat.dim()} \" + f\"{y_hat.shape}\"\n",
    "    C = y_hat.size(1)\n",
    "    if C == 2:\n",
    "        nr_mix = 1\n",
    "    else:\n",
    "        assert y_hat.size(1) % 3 == 0\n",
    "        nr_mix = y_hat.size(1) // 3\n",
    "\n",
    "    channel_dim = 1\n",
    "    # BxCxT -> BxTxC\n",
    "    y_hat = y_hat.transpose(1, 2)\n",
    "    channel_dim = 2\n",
    "\n",
    "    # unpack parameters.\n",
    "    if C == 2:\n",
    "        # special case for C == 2, just for compatibility\n",
    "        logit_probs = None\n",
    "        means = y_hat[:, :, 0:1]\n",
    "        log_scales = torch.clamp(y_hat[:, :, 1:2], min=log_scale_min) # clamp removable?\n",
    "    else:\n",
    "        #  (B, T, num_mixtures) x 3\n",
    "        logit_probs = y_hat[:, :, :nr_mix]\n",
    "        means = y_hat[:, :, nr_mix:2 * nr_mix]\n",
    "        log_scales = torch.clamp(y_hat[:, :, 2 * nr_mix:3 * nr_mix],\n",
    "                                 min=log_scale_min)\n",
    "\n",
    "    # repeat to match all the distributions in the mixture\n",
    "    # B x T x 1 -> B x T x num_mixtures\n",
    "    y = y.expand_as(means)\n",
    "\n",
    "    centered_y = y - means\n",
    "    dist = Normal(loc=0., scale=torch.exp(log_scales))\n",
    "\n",
    "    log_probs = dist.log_prob(centered_y)\n",
    "\n",
    "    if nr_mix > 1:\n",
    "        log_probs = log_probs + F.log_softmax(logit_probs, -1) # mixture probabilities are added: log P(x_{t=1}^{(i=1)})+log P_{jt}^{(i)}\n",
    "\n",
    "    if reduction == 'mean':\n",
    "        if nr_mix == 1:\n",
    "            # NLL\n",
    "            return -torch.mean(log_probs) # sum in every dimension (B, T, num_mixtures) and divide by the number of elements, return an scalar\n",
    "        else:\n",
    "#             return -torch.sum(log_sum_exp(log_probs))\n",
    "            return -torch.mean(torch.logsumexp(log_probs, channel_dim)) \n",
    "            # logsumexp in channel/mixtures dimension and then sum everything else\n",
    "    elif reduction == 'sum':\n",
    "        if nr_mix == 1:\n",
    "            # NLL\n",
    "            return -torch.sum(log_probs) # sum in every dimension (B, T, num_mixtures) and return an scalar\n",
    "        else:\n",
    "#             return -torch.sum(log_sum_exp(log_probs))\n",
    "            return -torch.sum(torch.logsumexp(log_probs, channel_dim))\n",
    "            # logsumexp in channel/mixtures dimension and then sum everything else\n",
    "    elif reduction == 'sumtime':\n",
    "        if nr_mix == 1:\n",
    "            return -torch.sum(log_probs.mean(0)) # sum in time mean in batch (B, T, num_mixtures) and return an scalar\n",
    "        else:\n",
    "            return -torch.sum(torch.logsumexp(log_probs, channel_dim).mean(0))\n",
    "    else:\n",
    "        if nr_mix == 1:\n",
    "            return -log_probs\n",
    "        else:\n",
    "#             return -log_sum_exp(log_probs).unsqueeze(-1)\n",
    "            return -torch.logsumexp(log_probs, channel_dim).unsqueeze(-1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20faa1a3-8f37-49b2-8685-3ab8810779ff",
   "metadata": {},
   "source": [
    "### Kullback-Leibler Divergence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c048512-6a8a-43c5-a3fc-59f152cee75e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#|export\n",
    "def kl_divergence(mu, logvar, reduction='meansum'):\n",
    "    \"\"\"\n",
    "    Compute the divergence term of the VAE loss function.\n",
    "    \"\"\"\n",
    "    klds = -0.5*(1. + logvar - mu.pow(2) - logvar.exp()) # batch_size, z_dim\n",
    "    if reduction == 'meansum':\n",
    "        # total_kld\n",
    "        kld = klds.sum(1).mean(0)\n",
    "        # sum of dimension_wise_kld # size 1 \n",
    "    elif reduction == 'mean':\n",
    "        kld = klds.mean()\n",
    "    elif reduction == 'sum':\n",
    "        kld = klds.sum()\n",
    "    elif reduction == 'meanb': # dimension_wise_kld\n",
    "        kld = klds.mean(0)\n",
    "    else:\n",
    "        kld = klds\n",
    "\n",
    "    # sanity check\n",
    "    assert torch.all(torch.isfinite(kld)), f\"Non finite value found on KLD loss computation: {kld.item()}.\"\n",
    "    f'May {logvar=} be too big/small?'\n",
    "    return kld"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "075899e6-5e40-414d-b763-f086bcd33de4",
   "metadata": {},
   "source": [
    "## Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a9855c4-e59a-471c-a995-8a8663e3f15d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#|export\n",
    "# inspired from https://github.com/EtienneT/vae/blob/master/vae.ipynb\n",
    "# and https://github.com/fastai/fastai/blob/master/fastai/learner.py#L467\n",
    "class GaussianMixtureMetric(AvgMetric):\n",
    "    \"\"\"Metric to log the Gaussian mixture loss\"\"\"\n",
    "    def __init__(self, RF:int,c,use_pad:bool=False,func=mix_gaussian_loss, reduction='mean'): store_attr()\n",
    "    def accumulate(self, learn):\n",
    "        bs = find_bs(learn.yb)\n",
    "        if   self.c > 0: pred, mu, logvar, c_ = to_detach(learn.pred, cpu=False) # VAE + WaveNet\n",
    "        elif self.c== 0: pred                 = to_detach(learn.pred, cpu=False) # WaveNet\n",
    "        elif self.c==-1: pred, mu, logvar     = to_detach(learn.pred, cpu=False) # VAE\n",
    "        gaussian_mix_loss = to_detach(self.func(pred, learn.y[:,self.RF:], reduction=self.reduction).mean(), cpu=True)\n",
    "        if self.use_pad:\n",
    "            gaussian_mix_loss = to_detach(self.func(pred, learn.y, reduction=self.reduction).mean(), cpu=True)\n",
    "        else:\n",
    "            gaussian_mix_loss = to_detach(self.func(pred, learn.y[:,self.RF:], reduction=self.reduction).mean(), cpu=True)\n",
    "        self.total += gaussian_mix_loss*bs\n",
    "        self.count += bs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "259cec0e-7a14-445e-be9d-fd759617c6d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#|export\n",
    "class KLDMetric(AvgMetric):\n",
    "    \"\"\"Metric to log the Kullback-Leibler divergence term\"\"\"\n",
    "    def __init__(self, c): store_attr()\n",
    "    def accumulate(self, learn):\n",
    "        bs = find_bs(learn.yb)\n",
    "        if   self.c > 0: pred, mu, logvar, c_ = to_detach(learn.pred, cpu=False) # VAE + WaveNet\n",
    "        elif self.c==-1: pred, mu, logvar     = to_detach(learn.pred, cpu=False) # VAE\n",
    "        KLD = -0.5*(1 + logvar - mu.pow(2) - logvar.exp()).sum(1).mean(0) if self.c and mu is not None else 0 # batch_size, z_dim\n",
    "        KLD = to_detach(KLD, cpu=True)\n",
    "        self.total += KLD*bs\n",
    "        self.count += bs\n",
    "    @property\n",
    "    def name(self):  return 'kld'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae9c8498-0731-42a0-b742-4f0f9b3d9ba9",
   "metadata": {},
   "source": [
    "## Learner Callbacks"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf2f23d3-5cc0-4838-b9a1-5c975917919f",
   "metadata": {},
   "source": [
    "Utils to get insights from training dynamics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a6132fc-983c-4071-8d26-4aeb75916a2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#|export\n",
    "class ShowLossCallback(Callback):\n",
    "    \"Update a graph of training and validation loss\"\n",
    "    order,run_valid=65,False\n",
    "    def __init__(self, title=''): self.title=title\n",
    "    def before_fit(self):\n",
    "        self.run = not hasattr(self.learn, 'lr_finder') and not hasattr(self, \"gather_preds\")\n",
    "        if not(self.run): return\n",
    "        self.nb_batches = []\n",
    "        assert hasattr(self.learn, 'progress')\n",
    "\n",
    "    def after_train(self):  self.nb_batches.append(self.train_iter)\n",
    "\n",
    "    def after_epoch(self):\n",
    "        \"Plot validation loss in the pbar graph\"\n",
    "        if not self.nb_batches: return\n",
    "        rec = self.learn.recorder\n",
    "        iters = range_of(rec.losses)\n",
    "        val_losses = [v[1] for v in rec.values]\n",
    "        # no bounds to see fluctuation in real time\n",
    "        x_bounds = None # (0, (self.n_epoch - len(self.nb_batches)) * self.nb_batches[0] + len(rec.losses)) # None\n",
    "        y_bounds = None # (min((min(Tensor(rec.losses)), min(Tensor(val_losses)))),\n",
    "                    # max((max(Tensor(rec.losses)), max(Tensor(val_losses)))))\n",
    "        self.progress.mbar.update_graph([(iters, rec.losses), (self.nb_batches, val_losses)], x_bounds, y_bounds)\n",
    "\n",
    "        self.progress.mbar.graph_ax.set_xlabel('Batches', x=0, horizontalalignment='right', verticalalignment='bottom')\n",
    "        self.progress.mbar.graph_ax.set_ylabel('Loss')\n",
    "        self.progress.mbar.graph_ax.set_title(self.title)\n",
    "        # Epochs x axis\n",
    "        # global batches_per_epoch\n",
    "        if self.nb_batches:\n",
    "            batches_per_epoch = self.nb_batches[0]# [-1]/self.n_epoch\n",
    "            def epoch2batch(epoch):    return epoch * batches_per_epoch\n",
    "            def batch2epoch(batch):    return batch / batches_per_epoch\n",
    "            secax = self.progress.mbar.graph_ax.secondary_xaxis(-0.2, functions=(batch2epoch, epoch2batch))\n",
    "            secax.set_xlabel('Epochs', x=0, horizontalalignment='right', verticalalignment='bottom');\n",
    "            self.progress.mbar.graph_out.update(self.progress.mbar.graph_ax.figure)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f206a71-4b54-4638-9ba6-78d5c86b9025",
   "metadata": {},
   "outputs": [],
   "source": [
    "#|export\n",
    "class ShowKLDsCallback(Callback):\n",
    "    \"Update a graph of training and validation loss\"\n",
    "    order,run_valid=65,False\n",
    "    names = ['']\n",
    "    def __init__(self, title=''):    self.title=title\n",
    "\n",
    "    def before_fit(self):\n",
    "        self.run = not hasattr(self.learn, 'lr_finder') and not hasattr(self, \"gather_preds\")\n",
    "        if not(self.run): return\n",
    "        self.nb_batches = []\n",
    "        assert hasattr(self.learn, 'progress')\n",
    "\n",
    "    def after_validate(self):  self.on_iter_end()\n",
    "\n",
    "    def after_fit(self):\n",
    "        self.on_iter_end()\n",
    "        if hasattr(self, 'graph_fig'): delattr(self, 'graph_fig')\n",
    "        # needed to recreate the graph when fitting again\n",
    "        # https://github.com/fastai/fastai/blob/master/fastai/callback/progress.py#L39\n",
    "\n",
    "    def after_train(self):    self.on_iter_end();    self.nb_batches.append(self.train_iter)\n",
    "\n",
    "    def on_iter_end(self):\n",
    "        # from https://github.com/fastai/fastprogress/blob/master/fastprogress/fastprogress.py#L155\n",
    "        if hasattr(self, 'imgs_fig'):\n",
    "            plt.close()\n",
    "            self.imgs_out.update(self.imgs_fig)\n",
    "        if hasattr(self, 'graph_fig'):\n",
    "            plt.close()\n",
    "            self.graph_out.update(self.graph_fig)\n",
    "\n",
    "    def after_epoch(self):\n",
    "        \"Plot validation loss in the pbar graph\"\n",
    "        if not self.nb_batches: return\n",
    "        klds = np.stack(self.learn.kl_ds.preds)\n",
    "        start_b = 0#17500\n",
    "        # plt.yscale('symlog', linthresh=1e-8);\n",
    "        # no bounds to see fluctuation in real time\n",
    "        x_bounds = None # (0, (self.n_epoch - len(self.nb_batches)) * self.nb_batches[0] + len(rec.losses)) # None\n",
    "        y_bounds = None # (min((min(Tensor(rec.losses)), min(Tensor(val_losses)))),\n",
    "                    # max((max(Tensor(rec.losses)), max(Tensor(val_losses)))))\n",
    "        graphs = [(range(start_b,len(klds)),klds[start_b:])]\n",
    "        self.update_graph(graphs, x_bounds, y_bounds)\n",
    "\n",
    "    def update_graph(self, graphs, x_bounds=None, y_bounds=None, figsize=(6,4)):\n",
    "        if not hasattr(self, 'graph_fig'):\n",
    "            self.graph_fig, self.graph_ax = plt.subplots(1, figsize=figsize)\n",
    "            self.graph_out = display(self.graph_ax.figure, display_id=True)\n",
    "        self.graph_ax.clear()\n",
    "        if len(self.names) < len(graphs): self.names += [''] * (len(graphs) - len(self.names))\n",
    "        for g,n in zip(graphs,self.names): self.graph_ax.plot(*g, label=n)\n",
    "        h,l = self.graph_ax.get_legend_handles_labels()\n",
    "        if h: self.graph_ax.legend(h,l,loc='upper right')\n",
    "        if x_bounds is not None: self.graph_ax.set_xlim(*x_bounds)\n",
    "        if y_bounds is not None: self.graph_ax.set_ylim(*y_bounds)\n",
    "\n",
    "        self.graph_ax.set_xlabel('Batches', x=0., horizontalalignment='right', verticalalignment='bottom')\n",
    "        self.graph_ax.set_ylabel(r'DKL')\n",
    "        self.graph_ax.grid()\n",
    "        self.graph_ax.set_yscale('log')\n",
    "        self.graph_ax.set_title(self.title)\n",
    "        # Epochs x axis\n",
    "        # global batches_per_epoch\n",
    "        if self.nb_batches:\n",
    "            batches_per_epoch = self.nb_batches[0]\n",
    "            def epoch2batch(epoch):    return epoch * batches_per_epoch\n",
    "            def batch2epoch(batch):    return batch / batches_per_epoch\n",
    "            secax = self.graph_ax.secondary_xaxis(-0.2, functions=(batch2epoch, epoch2batch))\n",
    "            secax.set_xlabel('Epochs', x=0., horizontalalignment='right', verticalalignment='bottom');\n",
    "        self.graph_out.update(self.graph_ax.figure)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d650c358-f166-4604-bf88-7c16d303de40",
   "metadata": {},
   "outputs": [],
   "source": [
    "#|export\n",
    "class ShowLatentsCallback(ShowKLDsCallback):\n",
    "    \"Update a graph of latent space\"\n",
    "    order,run_valid=65,False\n",
    "    names = ['']\n",
    "    def __init__(self, c, title=''): store_attr()\n",
    "\n",
    "    def before_fit(self):\n",
    "        self.run = not hasattr(self.learn, 'lr_finder') and not hasattr(self, \"gather_preds\")\n",
    "        if not(self.run): return\n",
    "        self.nb_batches = []\n",
    "        assert hasattr(self.learn, 'progress')\n",
    "        # get idx\n",
    "        alphas_items = self.learn.dls.valid.items[:,0]\n",
    "        Ds_items     = self.learn.dls.valid.items[:,1]\n",
    "        u_a=np.unique(alphas_items, return_index=True, return_inverse=True)\n",
    "        u_D=np.unique(Ds_items, return_index=True, return_inverse=True)\n",
    "        alphas_idx = [np.flatnonzero(alphas_items==a) for a in u_a[0]]\n",
    "        Ds_idx = [np.flatnonzero(Ds_items==D) for D in u_D[0]]\n",
    "\n",
    "        intersect_idx = []\n",
    "        for i,a in enumerate(u_a[0]):\n",
    "            for j,D in enumerate(u_D[0]):\n",
    "                intersect_idx.append(np.intersect1d(alphas_idx[i],Ds_idx[j]))\n",
    "\n",
    "        self.intersect_idx = np.array(intersect_idx).reshape(len(u_a[0]),len(u_D[0]))\n",
    "        self.alphas_idx_flat = [item for sublist in alphas_idx for item in sublist]\n",
    "        self.Ds_idx_flat = [item for sublist in Ds_idx for item in sublist]\n",
    "\n",
    "    def after_epoch(self):\n",
    "        \"Plot validation loss in the pbar graph\"\n",
    "        if not self.nb_batches: return\n",
    "\n",
    "        cb = GatherPredsCallback(with_targs=False)\n",
    "        ctx_mgrs = self.validation_context(cbs=L(cb), inner=True)\n",
    "        dl = self.learn.dls[1].new(shuffle=False, drop_last=False) if self.learn.dls[1].drop_last else self.learn.dls[1]\n",
    "        with ContextManagers(ctx_mgrs):\n",
    "            self._do_epoch_validate(dl=dl)\n",
    "            preds, _ = cb.all_tensors()\n",
    "        if self.c >0: pred, mu, logvar, c = to_detach(preds)\n",
    "        elif self.c==-1: pred, mu, logvar = to_detach(preds)\n",
    "\n",
    "        # plot\n",
    "        # no bounds to see fluctuation in real time\n",
    "        x_bounds = None; y_bounds = None\n",
    "        graphs = [(range(len(self.alphas_idx_flat)),mu[self.alphas_idx_flat])]\n",
    "        self.update_graph(graphs, x_bounds, y_bounds)\n",
    "\n",
    "    def update_graph(self, graphs, x_bounds=None, y_bounds=None, figsize=(6,4)):\n",
    "        if not hasattr(self, 'graph_fig'):\n",
    "            self.graph_fig, self.graph_ax = plt.subplots(1, figsize=figsize)\n",
    "            self.graph_out = display(self.graph_ax.figure, display_id=True)\n",
    "        self.graph_ax.clear()\n",
    "        if len(self.names) < len(graphs): self.names += [''] * (len(graphs) - len(self.names))\n",
    "        for g,n in zip(graphs,self.names): self.graph_ax.plot(*g, label=n)\n",
    "        h,l = self.graph_ax.get_legend_handles_labels()\n",
    "        if h: self.graph_ax.legend(h,l,loc='upper right')\n",
    "        if x_bounds is not None: self.graph_ax.set_xlim(*x_bounds)\n",
    "        if y_bounds is not None: self.graph_ax.set_ylim(*y_bounds)\n",
    "\n",
    "        self.graph_ax.set_xlabel('ordered a index',)\n",
    "        self.graph_ax.set_ylabel(r'\\(\\mu\\) latent neuron activation')\n",
    "        self.graph_ax.grid()\n",
    "        # self.graph_ax.set_yscale('log')\n",
    "        self.graph_ax.set_title(self.title)\n",
    "        self.graph_out.update(self.graph_ax.figure)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd530d50-ef79-4a8e-b76b-c6ca6920a0ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "#|export\n",
    "class KLDsCallback(Callback):\n",
    "    \"Record KLD per latent variable\"\n",
    "    order,run_valid=65,False\n",
    "    def __init__(self,c): store_attr(); self.preds = []\n",
    "    def after_batch(self):\n",
    "        if   self.c > 0: pred, mu, logvar, c_ = to_detach(self.learn.pred, cpu=False) # VAE + WaveNet\n",
    "        elif self.c==-1: pred, mu, logvar     = to_detach(self.learn.pred, cpu=False) # VAE\n",
    "        KLD = -0.5*(1 + logvar - mu.pow(2) - logvar.exp()).mean(0) if mu is not None else 0. # z_dim\n",
    "        self.preds.append(KLD) # z_dim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37587298-94ff-4345-b081-2d844230dcda",
   "metadata": {},
   "outputs": [],
   "source": [
    "#|export\n",
    "def plot_klds(learn, start_b=0, title=''):\n",
    "    klds = learn.model.beta*np.stack(learn.kl_ds.preds) if learn.model.beta!=0. else np.stack(learn.kl_ds.preds)\n",
    "    plt.semilogy(range(start_b,len(klds)),klds[start_b:]);\n",
    "    plt.xlabel('Batches');plt.ylabel(r'\\(\\beta\\) DKL');\n",
    "    plt.title(title);\n",
    "    plt.grid();\n",
    "    if learn.model.beta>0. and hasattr(learn, 'loss_param_scheduler'): plt.plot(learn.loss_param_scheduler.hps['beta'], alpha=0.5);\n",
    "    plt.yscale('symlog', linthresh=1e-8);\n",
    "    return plt.gcf()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6fc733a-9bcc-4c4c-901a-40a34243cff8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#|export\n",
    "class GMsCallback(Callback):\n",
    "    \"Record NLL gaussian mixture log-likelihood means per alpha and D during training\"\n",
    "    order,run_valid=65,False\n",
    "    def __init__(self,RF,c,use_pad=False,reduction='none'): store_attr(); self.preds = [];\n",
    "    def after_pred(self):\n",
    "        if   self.c > 0: pred, mu, logvar, c_ = to_detach(self.learn.pred, cpu=False) # VAE + WaveNet\n",
    "        elif self.c== 0: pred                 = to_detach(self.learn.pred, cpu=False) # WaveNet\n",
    "        elif self.c==-1: pred, mu, logvar     = to_detach(self.learn.pred, cpu=False) # VAE\n",
    "        if self.use_pad:\n",
    "            gaussian_mix_loss = mix_gaussian_loss(pred, self.learn.y, reduction=self.reduction)\n",
    "        else:\n",
    "            gaussian_mix_loss = mix_gaussian_loss(pred, self.learn.y[:,self.RF:], reduction=self.reduction)\n",
    "        gaussian_mix_loss = to_detach(gaussian_mix_loss, cpu=True)\n",
    "        # get idx\n",
    "        i = self.learn.iter # current iteration, batch number\n",
    "        current_items = self.learn.dls.items[ self.learn.dls.train._DataLoader__idxs[i*self.learn.dls.bs:(i+1)*self.learn.dls.bs]]\n",
    "        alphas_items, Ds_items = current_items[:,0], current_items[:,1]\n",
    "        u_a=np.unique(alphas_items);  u_D=np.unique(Ds_items)\n",
    "        alphas_idx = [np.flatnonzero(alphas_items==a) for a in u_a]\n",
    "        Ds_idx = [np.flatnonzero(Ds_items==D) for D in u_D]\n",
    "\n",
    "        intersect_idx = np.array([[np.intersect1d(alphas_idx[i],Ds_idx[j]) for j,D in enumerate(u_D)] for i,a in enumerate(u_a)]\n",
    "                                 ,dtype=object\n",
    "                                )\n",
    "\n",
    "        gaussian_mix_loss_mean_aD = np.array([\n",
    "            [gaussian_mix_loss[intersect_idx[i,j]].mean()\n",
    "             for j,D in enumerate(u_D)]\n",
    "            for i,a in enumerate(u_a)] )\n",
    "\n",
    "        self.preds.append(gaussian_mix_loss_mean_aD)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1147c4f1-cdfd-4beb-9f37-e0d89e3b14f3",
   "metadata": {},
   "source": [
    "## Save/Load models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5df5fc9a-bc45-406a-8eeb-44ed37f55bef",
   "metadata": {},
   "outputs": [],
   "source": [
    "#|export\n",
    "def save_model(fpath, model, model_args, ds_args,):\n",
    "    state = dict(model_args=model_args, ds_args=ds_args,\n",
    "                 network_state_dict=model.state_dict(),)\n",
    "    torch.save(state, f'{fpath}.tar')\n",
    "    print('Saved at '+ f'{fpath}.tar')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ceed4a9d-53b0-4f95-9948-a66424da4144",
   "metadata": {},
   "outputs": [],
   "source": [
    "#|export\n",
    "def load_checkpoint(fpath, model_class=VAEWaveNet, device='cuda'):\n",
    "    try: # load into device\n",
    "        checkpoint = torch.load(f'{fpath}.tar', map_location=device)\n",
    "    except: # use CPU\n",
    "        device = 'cpu' \n",
    "        checkpoint = torch.load(f'{fpath}.tar', map_location=device)\n",
    "        \n",
    "    print(\"Loading checkpoint: \"+f\"{fpath}.tar\" + f'\\non device: {device}')\n",
    "\n",
    "    network = model_class(**checkpoint['model_args']).to(device)\n",
    "    try:\n",
    "        network.load_state_dict(checkpoint['network_state_dict'])\n",
    "    except:\n",
    "        raise RuntimeError(\"Error loading the checkpoint\")\n",
    "    return checkpoint, network"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0888376a-1e55-4b65-a059-d2023ea66374",
   "metadata": {},
   "source": [
    "# Anomalous Diffusion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8f5e2dd-6988-43eb-9c98-f74c19d6c79a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#|export\n",
    "def sig2D(sigma):\n",
    "    \"\"\"Converts standard deviation into the associated diffusion coefficient\n",
    "    $D=\\sigma^2/2$ \n",
    "    \"\"\"\n",
    "    return sigma**2*0.5\n",
    "def D2sig(D):\n",
    "    \"\"\"Converts standard deviation into the associated diffusion coefficient\n",
    "    $\\sigma=\\sqrt{2D}$\n",
    "    \"\"\"\n",
    "    return (2*D)**0.5"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
